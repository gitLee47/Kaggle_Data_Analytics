{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 131 entries, cat1 to loss\n",
      "dtypes: float64(15), object(116)\n",
      "memory usage: 188.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/aligator4sah/Kaggle_Data_Analytics/blob/master/data/test.csv\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "#%matplotlib inline \n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    " \n",
    "train_data = pd.read_csv(\"C:/Study/kaggle_Pro/train.csv\") \n",
    "test_data = pd.read_csv(\"C:/Study/kaggle_Pro/test.csv\")  \n",
    " \n",
    "#print(\"Train data dimensions: \", train_data.shape) \n",
    "#print(\"Test data dimensions: \", test_data.shape) \n",
    "train_data = train_data.iloc[:,1:]  \n",
    " \n",
    "train_data.info() \n",
    "test_data.info() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Updating loss \n",
    "train_data['loss'] = np.log1p(train_data[\"loss\"])  \n",
    " \n",
    "# sepearte the categorical and continous features \n",
    "cont_columns = [] \n",
    "cat_columns = []  \n",
    " \n",
    "for i in train_data.columns: \n",
    "    if train_data[i].dtype == 'float': \n",
    "        cont_columns.append(i) \n",
    "    elif train_data[i].dtype == 'object': \n",
    "        cat_columns.append(i) \n",
    " \n",
    "for cf1 in cat_columns: \n",
    "    le = LabelEncoder() \n",
    "    le.fit(train_data[cf1].unique()) \n",
    "    train_data[cf1] = le.transform(train_data[cf1]) \n",
    " \n",
    "#train_data.head(20) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntrain = train_data.shape[0] \n",
    "ntest = test_data.shape[0]  \n",
    " \n",
    "features = [x for x in train_data.columns if x not in ['id','loss']]  \n",
    " \n",
    "train_test = pd.concat((train_data[features], test_data[features])).reset_index(drop=True) \n",
    " \n",
    "#train_test = np.concatenate((train_data_pred[features],test_data[features]),axis=0) \n",
    " \n",
    "for f in train_test.columns:  \n",
    "    if train_test[f].dtype=='object':  \n",
    "        lbl = LabelEncoder()  \n",
    "        lbl.fit(list(train_test[f].values))  \n",
    "        train_test[f] = lbl.transform(list(train_test[f].values)) \n",
    " \n",
    "train_x = train_test.iloc[:ntrain,:] \n",
    " \n",
    "test_x = train_test.iloc[ntrain:,:] \n",
    "    \n",
    "train_x = np.array(train_x); \n",
    "test_x = np.array(test_x); \n",
    "#print(test_x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daisy\\AppData\\Roaming\\Python\\Python27\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#get the number of rows and columns \n",
    "r, c = train_x.shape \n",
    " \n",
    "#Y is the target column, X has the rest \n",
    "X = train_x \n",
    "Y = train_data['loss'] \n",
    " \n",
    "#print X.shape \n",
    "print ntrain \n",
    " \n",
    "#Validation chunk size \n",
    "val_size = 0.4 \n",
    " \n",
    "#Use a common seed in all experiments so that same chunk is used for validation \n",
    "seed = 0  \n",
    " \n",
    "from sklearn import cross_validation \n",
    "X_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed) \n",
    " \n",
    "del X \n",
    "del Y \n",
    " \n",
    "i_cols = [] \n",
    "for i in range(0,c-1): \n",
    "    i_cols.append(i) \n",
    " \n",
    "#All features \n",
    "X_all = [] \n",
    " \n",
    "#List of combinations \n",
    "comb = [] \n",
    " \n",
    "#Dictionary to store the MAE for all algorithms  \n",
    "mae = [] \n",
    " \n",
    "#Scoring parameter \n",
    "from sklearn.metrics import mean_absolute_error \n",
    " \n",
    "#Add this version of X to the list  \n",
    "n = \"All\" \n",
    "#X_all.append([n, X_train,X_val,i_cols]) \n",
    "X_all.append([n, i_cols]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1295.70094426\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of various combinations of LinearRegression \n",
    " \n",
    "#Import the library \n",
    "from sklearn.linear_model import LinearRegression \n",
    "import numpy as numpy\n",
    " \n",
    "##Set the base model \n",
    "model = LinearRegression(n_jobs=-1) \n",
    "algo = \"LR\" \n",
    "# \n",
    "##Accuracy of the model using all features \n",
    "for name,i_cols_list in X_all: \n",
    "    model.fit(X_train[:,i_cols_list],Y_train) \n",
    "    result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list]))) \n",
    "    mae.append(result) \n",
    "    print(name + \" %s\" % result) \n",
    "comb.append(algo) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1295.06901455\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of various combinations of ElasticNet LinearRegression\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#When alpha equals to 0.001,I get the lowest MAE.\n",
    "a_list = numpy.array([0.001])\n",
    "\n",
    "for alpha in a_list:\n",
    "    #Set the base model\n",
    "    model = ElasticNet(alpha=alpha,random_state=seed)\n",
    "    \n",
    "    algo = \"Elastic\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % alpha )\n",
    "\n",
    "#all the non-liner regression model get worse result than liner regression, so I didn't try here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1232.13382429\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of various combinations of RandomForest\n",
    "import numpy as numpy\n",
    "#Import the library\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#When the n_estimator equals to 50, we can get the lowest MAE.\n",
    "n_list = numpy.array([50])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = RandomForestRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\n",
    "    \n",
    "    algo = \"RF\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n",
    "\n",
    "#will try the extremly randomized tree later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1226.19538847\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of various combinations of Bagged Decision Trees\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "n_list = numpy.array([100])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = BaggingRegressor(n_jobs=-1,n_estimators=n_estimators)\n",
    "    \n",
    "    algo = \"Bag\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1608.15856307\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of various combinations of AdaBoost\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "#When the n_estimator equals to 100. I get the lowest MAE.\n",
    "n_list = numpy.array([100])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = AdaBoostRegressor(n_estimators=n_estimators,random_state=seed)\n",
    "    \n",
    "    algo = \"Ada\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n",
    "\n",
    "#The Mae is too high so I don't think Adaboost is fitful for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1185.27819913\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of various combinations of SGB (Sophisticated Gradient Boosting)\n",
    "\n",
    "#Import the library\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#I tried n_estimator from 50-250 and I found the higher n_estimator, the lower MAE I got.\n",
    "n_list = numpy.array([250])\n",
    "\n",
    "for n_estimators in n_list:\n",
    "    #Set the base model\n",
    "    model = GradientBoostingRegressor(n_estimators=n_estimators,random_state=seed)\n",
    "    \n",
    "    algo = \"SGB\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_estimators )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1702.61267621\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of various combinations of KNN\n",
    "\n",
    "#Import the library\n",
    "import numpy\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#Add the N value to the below list if you want to run the algo\n",
    "n_list = numpy.array([10])\n",
    "\n",
    "for n_neighbors in n_list:\n",
    "    #Set the base model\n",
    "    model = KNeighborsRegressor(n_neighbors=n_neighbors,n_jobs=-1)\n",
    "    \n",
    "    algo = \"KNN\"\n",
    "\n",
    "    #Accuracy of the model using all features\n",
    "    for name,i_cols_list in X_all:\n",
    "        model.fit(X_train[:,i_cols_list],Y_train)\n",
    "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
    "        mae.append(result)\n",
    "        print(name + \" %s\" % result)\n",
    "        \n",
    "    comb.append(algo + \" %s\" % n_neighbors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All the preperations I used the same way as Liju did.\n",
    "\n",
    "#I also tried MLP on my laptop, but it keeps running for an hour and nothing happened. \n",
    "#I'm wondering if there is any dead loop inside the MLP.\n",
    "#I think we can check it next time when we meet.\n",
    "\n",
    "#In conclusion, SGB get the best result and Adaboost get the worst result.\n",
    "#For regression and bagging, their result are just so so but I think we can make a change by diff ways of combination.\n",
    "#Most of the model above, I used all features to fit them.\n",
    "#I think we should find some ways to identify which model fit for which features and combine them.\n",
    "#Second thing is that we should know how to tune the parameter for each model. \n",
    "#Once we decide what models we are going to use, we can divide the model to each of us and combine them at last."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
